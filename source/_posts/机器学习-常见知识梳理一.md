---
title: 【机器学习】常见知识梳理（一）
date: 2020-08-03 09:10:23
categories:
	- 机器学习
tags:
	- 神经网络
---

各种包的存在已经使得生成一个机器学习代码十分方便，但是当遇到一个个分厂常见的名词时，可能还是需要从理论上了解一番。

这里先不指定具体的框架，因为不论是tensorflow、keras、pytorch、caffe、mxnet几乎都会有这些内容，区别可能是名称或者方法的具体路径不同。

<!--more-->

## 模型

### 层-layer-积木

**dense层**或fully connect（FC）层

- 全连接层

flatten扁平层

- 多维的输入一维化，常用在从卷积层到全连接层的过渡，不影响batch的大小

softmax层

- 柔性最大化层，将神经网络的输出结果转化为概率分布，从而便于使用交叉熵

卷积层conv

- **卷积层的内容涉及卷积神经网络，可以单独说，这一部分比较多**

batch normaliazation（BN）层

- 归一化层

此外还有池化层 pooling layer等，这些层就是构成模型的基础层，绝大多数神经网络都建立在这些层的组合之上。



### 优化方法-optimizer

神经网络普遍使用**“梯度下降-Gradient Descent”**的算法来对网络中的参数进行迭代优化。

可以这样想象：如果没有优化，网络中的参数就是静态的，那可能不是最优化的；经典案例——下山的例子告诉我们，按照最大梯度方向走出一定步长就可以实现下山，从而找到我们想要的山谷点；如果网络参数不是最大梯度方向，那我们就可以根据误差值来修正网络参数，从而使其更加准确。

> 梯度爆炸和梯度消失问题

有三种常用的优化方法：随机梯度下降-SGD、RMSprop、Adam。

它们的原理稍有些复杂，好在大多数框架已经进行了封装，可以先进行使用尝试，后续再谈。



### 损失函数-loss function

也可以叫误差函数，成本函数，代价函数（cost function），总之是表示神经网络计算结果与真实结果之间的误差的一种函数。

如果只是数字，当然可以直接比两个数字是不是很接近，但是当人们发现评价一个模型的loss或者cost也需要加入用时甚至使用的空间时，代价函数的形式就需要进行调整了。

常用的loss function有：

- MSE：（预测值和真实值之间）均方误差

- binary cross-entropy：二分交叉熵，也可以叫二分对数损失；适合二元标签预测

- categorical cross-entropy： 多分类对数损失，分类交叉熵，多分类标签预测，是与激活函数softmax关联的默认选择

  

### 反向传播算法-Back propagation

神经网络的精髓。

反向传播的数学原理就是数学中的链式法则。

![链式法则](https://pic3.zhimg.com/v2-14ae4c7f3ed45de439d457471bcb6b7a_1440w.jpg?source=172ae18b)

反向传播的目的是不断优化网络中的参数。

## 改进

### padding-加边

- stride-步长
- filter

padding的目的之一是保持数据大小的稳定，添加一些没有信息的边界，也扩大了输出的大小；

目的之二是充分利用边界信息。

### pooling-池化

- 常见的有最大池化（Max Pooling）和平均池化（Average Pooling）方法

  ![image-20200804093720762](https://typoraim.oss-cn-shanghai.aliyuncs.com/image/image-20200804093720762.png)

- 这是一种有效压缩图像信息的方法

### dropout-丢弃

神经网络和人脑不同的地方在于，人脑目前看来是真正的概率性，神经网络在计算机中其实是确定的程序。

为了增加神经网络的泛化能力，我们可以随机地丢弃一部分网络（不会影响总体的结构），如果这样的**网络仍能正确分类或者决策**，那岂不是说明**该网络比原网络有更强的泛化能力**，实验证明Dropout的处理方式有利于提高模型的准确性和泛化能力。

## 训练

模型的训练需要大量的数据，也就需要很强的算力，这也是为什么神经网络的思想早已出现，但直到几年前才开始爆发式发展的原因之一。

目前训练数据的基本思路是：

- 把一组完整的大量的数据分成两部分，一部分数据量大一点，作为训练集（training data），一部分数据量小一点，作为测试集（test data）

- 训练数据还要分成一个个小的批次，可以认为是batch，每一个batch中的数据多少，是batch size

  > 之所以这样做，一方面是因为数据训练时会进入内存，然后分配到CPU或者GPU中进行运算，如果进入内存的数据过多，可能会影响计算速度；另一方面，如果一次性把所有数据进行计算，相当于一次计算只更新了一次模型参数，其实收益很小。
  >
  > 如果分成小批量进行计算，每计算一次就更新了一次模型参数，这样比原来能多更新几次，增加了训练的收益。

- 把所有的训练集数据批运算完一遍，称作一个epoch，可以设定多个epoch，这样就可以不断进行迭代优化

- 最后再拿出宝贵的测试集数据，用模型测试一下，获得准确度。

怎么划分训练集和测试集、如何定义批量大小和epoch大小，也是神经网络优化的一个大问题。

总体来说，划分训练集和测试集越随机越好，模型准确度随批量大小和epoch大小的变化则是先升后降。

毕竟就算把**训练集**训练无数遍，甚至能默写，这也不是想要的学习，还可能产生**过拟合**的问题，提高**泛化能力**才是真正的学习。