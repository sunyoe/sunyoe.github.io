---
title: 【机器学习】常见知识梳理（二）
date: 2020-08-22 09:04:49
categories:
	- 机器学习
---

本文内容根据《有趣的机器学习》一书整理而成，可作为简单了解。

## 目前的机器学习分类

**监督学习**

- 有数据和标签

**无监督学习**

- 只有数据没有标签

**半监督学习**

- 少量数据有标签
- 大量数据无标签

**强化学习（Reinforcement learning）**

- 从经验中总结提升

**遗传算法（Genetic algorithm）**

- 适者生存，不适者淘汰，优中选优

<!--more-->

## 神经网络

一种统计学模型，能够不断学习

### 框架

**神经层**

- 输入层
  - 计算机能看到的全是数字
- 隐藏层
  - 信息加工处理
- 输出层

**神经元（节点）**

- 每个神经元都有它自己的刺激函数
- 有的神经元被激活，有的没有被激活
- 反向学习后改动神经元激活情况，有的变迟钝，有的被激活，对重要信息进行反映

### 学习经验

认识预测结果和真实结果之间的差别（错误偏差）；

然后把差别反向的方向上改动一点点。

### 技巧

**如何评价**

- 训练数据（70%）和测试数据（30%）
- 用测试数据来评价
- 从误差值开始评价
- 误差曲线和精确度曲线
- R2 score 回归问题的精度
- f1 score 不均匀
- 过拟合
  - 训练误差和测试误差
  - 正则化
- 交叉验证，调参

**特征标准化（使数据跨度尽量统一）**

**什么是好特征**

- 更容易辨别事物的特征（有区分能力）
- 避免无意义的信息
- 避免重复性的信息
- 避免复杂的信息

**激励函数**

- 不能用线性方程所概括的问题

- y=AF（Wx）激励函数

  要确保激励函数必须是可以微分的这样才能用微分把误差传递回去

  - **relu 函数**
    - 卷积中推荐
    - 循环中推荐
    - 负实轴上的值全为0
  - **sigmoid 函数**
    - 当横轴非常小时，函数值为0
    - 当横轴非常大时，函数值为1
  - **tanh 函数**
    - 循环中推荐

**过拟合 overfitting**

- 增加数据量
- 正则化
  - 惩罚机制

**不均衡数据**

- 比如数据有多数派和少数派，机器预测会选择多数派
- 避免这种情况的方法
  - 获取更多的数据
  - 换一种评价方式
  - 重组数据<复制少数并合成，砍掉一些多数>
  - 修改算法<如sigmoid函数调整门槛位置，原门槛大约是±1>

**批标准化（Batch Normalization）**

- Batch 把数据变成小批，对每一层进行正规化处理
- 还进行反向Normalization，让NN自己学习是否起到作用

**L1/L2正规化<针对过拟合问题>**

- 方法是在cost函数中除了样本误差部份外，再增加 theta 参数平方和项
- 如果拟合结果的非线性太强，theta 的平方和会比较大，惩罚机制

**加速训练过程**

- SGD
- adagrad
- monument
- RMSPROP
- adam

### 卷积神经网络（Convolutional Neural Networks）

图像识别，自然语言处理

**卷积**

- 不是对每个像素，而是对像素块进行处理，加强了对图像的识别

**批量过滤器**

- 不断移动，搜集像素块

像素块->边缘信息->局部物体->物体全部

**图片有高度**

- 黑白高度0和1
- 彩色图片颜色信息就是高度
- 每搜索过滤一次，图片长宽变小，深度加深

**pooling 池化**

- 防止丢失信息
- 池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量

**分类器**

**ReLU 线性整流单元**

### 循环神经网络（RNN）

可以用人工智能编写一首歌

顺序排列

数据间的关联，存储记忆

**弊端**

- 有序数据
- 健忘
  - 梯度消失，梯度弥散
    - 当信息出现在开始时刻，由于误差反向计算时有加权，当权重比较小时，在开始时刻的误差接近于0，相当于没有误差，产生了错误
  - 梯度爆炸
    - 这次是由于权重大于1，累积到开始时刻时，误差已经很大了

**LSTM RNN**

延缓记忆消失

- 输入控制
- 忘记控制
- 输出控制

### 自编码（Autoencoder）

如何用神经网络指导非监督学习

**是一种神经网络形式**

- 图片-打码-解码
- 图片-压缩（信息小但全面，有代表性）-解压
  - 压缩获取原数据的精髓
  - 求出误差，反向传递

**只用了数据，没有用标签，所以是非监督学习**

**PCA 原数据主程序分析**

### GAN 生成对抗网络

凭空捏造数据

**新手画家（generator）凭借随机数生成有用数据**

**（给于一定的数据标签）新手鉴赏家（discriminator）（学习判断哪些是真实数据，哪些是生成数据，然后将经验反向传递给generator教它如何用生成生成数据能更像真实数据）-新手画家**

### 强化学习

从什么都没有到学习达到目标的方法

#### 基本过程

评分-高分/低分-机器有分数导向性，会自动选择更多的高分动作；

也就是进行数据尝试，然后获得数据标签，反过来激励机器多做高分动作。

#### 大致分类

**基于**

- value-based 基于价值
  - Q-Learning
  - Actor-Critic
- 基于动作/基于概率
  - Policy Gradients

**环境区分**

- 理解环境
  - 按部就班，在自己所处的环境下解决问题
- 不理解环境
  - 多一个虚拟环境的建模过程
  - 想象力更好

**更新**

- 回合更新
  - 每局结束之后再更新
- 单步更新
  - 每结束一步就更新一次
  - Actor-Critic 演员评论家

**学习方法**

- 在线学习
  - 本人在场，亲自学习
  - Sarsa / Sarsa（lamda）
- 离线学习
  - 看别人玩，从中学习
  - 比如 Alpha Go

#### 典型强化学习方法

**Q-Learning**

- Q表决策，离线学习
- 估计（预测）的动作不一定是接下来要做的动作
- 估计动作按Q表，但是做动作还是随机的

**Sarsa**

- 在线学习
- 估计的动作也是实际的动作

**Sarsa（lamda）**

- lamda 衰变值
  - 离宝藏越近的脚步越重要（更新力度就要大）
  - 离宝藏越远，由于衰变累积，更新就会小一点
- 比如Sarsa（0）
  - 每走一步更新一次<单步更新>
- Sarsa（n）
  - 走完更新<回合更新>

**Deep Q Network**

- Q 现实 和 Q 估计
- 记忆库 + 现实网络 + 估计网络
- 可以自己玩电子游戏

**Policy Gradient**

- 基于动作，尤其是连续动作，但是输出值很多
- 随机筛选
- 没有误差，但是有反向传递（这句话似乎是错误的，但是莫烦确实这样说）
- 输出值-奖惩-加大被选可能（现实中的奖惩会左右更新过程）

**Actor-Critic 演员评论家**

- 基于价值
- 学习奖惩，与环境之间的奖惩
- 有一个问题是，基于奖惩有可能会左右摇摆，Actor这边有自己的神经网络，学习了方法，Critic这边也有
- 然后学一步，高分》》演员-低分》》评论家-高分···如此反复

**Deep Deterministic Policy Gradient**

- 更高级的方法，避免再左右中学不到东西
- 连续动作上更有效的学习，只有一个输出值
- 是 Actor-Critic 和 DQN 的加合

**AAA-C（A3-C）**

- 平行方式训练 Actor-Critic
- 经验分享
- 多核并行运算